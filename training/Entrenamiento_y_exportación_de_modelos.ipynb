{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPxgLCkN7kxrY143emT0/7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Molter73/proyecto-computacion-1/blob/mauro%2Fcontainers/training/Entrenamiento_y_exportaci%C3%B3n_de_modelos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmM-CN4oSJvk"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==v4.6.3\n",
        "\n",
        "![ ! -d /content/SemEval2024-Task8 ] && gdown --folder https://drive.google.com/drive/folders/14DulzxuH5TDhXtviRVXsH5e2JTY2POLi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-optimize"
      ],
      "metadata": {
        "id": "ZB6e6Q-pU35p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, ComplementNB\n",
        "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.pipeline import Pipeline\n",
        "import logging"
      ],
      "metadata": {
        "id": "Q25cDFPwU5YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset function\n",
        "def read_dataset(inFile):\n",
        "    print(\"\\nReading:\", inFile)\n",
        "    data =  pd.read_json(inFile, lines=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "oCsqQ5Q4U6uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subtask = \"B\"\n",
        "\n",
        "if subtask == \"A\":\n",
        "  # data paths and config\n",
        "  inTrain = '/content/SemEval2024-Task8/SubtaskA/subtaskA_train_monolingual.jsonl'\n",
        "  inTest = '/content/SemEval2024-Task8/SubtaskA/subtaskA_dev_monolingual.jsonl'\n",
        "  inDatasetTest = '/content/mount/dataset.jsonl'\n",
        "\n",
        "  max_instances_per_class = 20000\n",
        "  target = \"label\"\n",
        "elif subtask == \"B\":\n",
        "  # data paths and config\n",
        "  inTrain = '/content/SemEval2024-Task8/SubtaskB/subtaskB_train.jsonl'\n",
        "  inTest = '/content/SemEval2024-Task8/SubtaskB/subtaskB_dev.jsonl'\n",
        "  inDatasetTest = '/content/mount/dataset.jsonl'\n",
        "\n",
        "  max_instances_per_class = 9000\n",
        "  target = \"model\"\n",
        "else:\n",
        "  logging.error(\"Wrong subtask: {}. It should be A or B\".format(subtask))\n",
        "  raise ValueError(\"Wrong subtask: {}. It should be A or B\".format(subtask))"
      ],
      "metadata": {
        "id": "AI8GhHFHU8iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data paths and config\n",
        "\n",
        "max_features = 20000 # maximum number of features extracted for our instances\n",
        "random_seed = 777 # set random seed for reproducibility"
      ],
      "metadata": {
        "id": "I1KsgzgrU-rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "train_df = read_dataset(inTrain)\n",
        "test_df = read_dataset(inTest)\n",
        "dataset_df = read_dataset(inDatasetTest)"
      ],
      "metadata": {
        "id": "hAh_Ucw5VBy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downsample training data to train faster\n",
        "train_df = train_df.groupby(target).sample(n=max_instances_per_class, random_state=random_seed)"
      ],
      "metadata": {
        "id": "Clp0IFtqVD6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df[\"text\"]\n",
        "X_test = test_df[\"text\"]\n",
        "X_dataset = dataset_df[\"text\"]"
      ],
      "metadata": {
        "id": "NEJGMNXgVFkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize labels : from text to numeric vectors\n",
        "le = LabelEncoder()\n",
        "Y_train = le.fit_transform(train_df[target])\n",
        "Y_test = le.transform(test_df[target])\n",
        "Y_dataset = le.transform(dataset_df[target])"
      ],
      "metadata": {
        "id": "wU9gi6XxVI_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if subtask == \"A\":\n",
        "  id2label = ['human', 'machine']\n",
        "else:\n",
        "  id2label = le.classes_"
      ],
      "metadata": {
        "id": "uuMNvRV8iE9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "  'models': {\n",
        "    'BernoulliNB': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=True)),\n",
        "          ('clf', BernoulliNB(alpha=0.01, fit_prior=True)),\n",
        "        ]),\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 1), use_idf=False)),\n",
        "          ('clf', MultinomialNB(alpha=1.0, fit_prior=True)),\n",
        "        ]),\n",
        "    },\n",
        "    'ExtraTreesClassifier': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=True)),\n",
        "          ('clf', ExtraTreesClassifier(criterion='entropy', n_estimators=100)),\n",
        "        ]),\n",
        "    },\n",
        "    'PassiveAggressiveClassifier': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=False)),\n",
        "          ('clf', PassiveAggressiveClassifier(C=0.5, max_iter=2000)),\n",
        "        ]),\n",
        "    },\n",
        "    'LinearSVC': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=False)),\n",
        "          ('clf', LinearSVC(max_iter=1000, penalty='l2')),\n",
        "        ]),\n",
        "    },\n",
        "    'NearestCentroid': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=False)),\n",
        "          ('clf', NearestCentroid()),\n",
        "        ]),\n",
        "    },\n",
        "    'Perceptron': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=False)),\n",
        "          ('clf', Perceptron(alpha=0.0001, max_iter=1000, penalty='l1')),\n",
        "        ]),\n",
        "    },\n",
        "    'ComplementNB': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=False)),\n",
        "          ('clf', ComplementNB(alpha=0.01, norm=True,)),\n",
        "        ]),\n",
        "    },\n",
        "    'KNeighborsClassifier': {\n",
        "        'pipeline': Pipeline([\n",
        "          ('tfidf', TfidfVectorizer(max_features=max_features, stop_words=\"english\", ngram_range=(1, 2), use_idf=False)),\n",
        "          ('clf', KNeighborsClassifier(metric='euclidean')),\n",
        "        ]),\n",
        "    },\n",
        "  },\n",
        "  \"labels\": id2label,\n",
        "}"
      ],
      "metadata": {
        "id": "GPXdo5W8VOlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, values in models['models'].items():\n",
        "  pipeline = values['pipeline']\n",
        "  pipeline.fit(X_train, Y_train)\n",
        "\n",
        "  y_pred = pipeline.predict(X_test)\n",
        "  score = f1_score(Y_test, y_pred, average=\"macro\")\n",
        "\n",
        "  print(f\"Model: {name} Macro F1: {score}\")\n",
        "  print(classification_report(Y_test, y_pred, target_names=id2label))\n",
        "\n",
        "  y_pred = pipeline.predict(X_dataset)\n",
        "  score = f1_score(Y_dataset, y_pred, average=\"macro\")\n",
        "\n",
        "  print(f\"Model: {name} Dataset Macro F1: {score}\")\n",
        "  print(classification_report(Y_dataset, y_pred, target_names=id2label))"
      ],
      "metadata": {
        "id": "LrQHVi7tYPc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "os.makedirs(f'/content/mount/models{subtask}', exist_ok=True)\n",
        "\n",
        "for name, values in models['models'].items():\n",
        "  with open(f'/content/mount/models{subtask}/{name}.pkl', 'wb') as f:\n",
        "    pickle.dump(values['pipeline'], f)\n",
        "\n",
        "with open(f'/content/mount/labels{subtask}.pkl', 'wb') as f:\n",
        "  pickle.dump(models['labels'], f)"
      ],
      "metadata": {
        "id": "zCruyYHFZcPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4IRwuMsZ_GR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}