{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0h184USR9kgE2i4gVmtwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Molter73/proyecto-computacion-1/blob/mauro%2Fnotebooks/Entrenamiento_validaci%C3%B3n_cruzada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXNznhFrxXjP"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==v4.6.3\n",
        "\n",
        "![ ! -d /content/SemEval2024-Task8 ] && gdown --folder https://drive.google.com/drive/folders/14DulzxuH5TDhXtviRVXsH5e2JTY2POLi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-optimize"
      ],
      "metadata": {
        "id": "QFP2yzR2xZsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U-kYg4rlxcdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, ComplementNB\n",
        "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.pipeline import Pipeline\n",
        "import logging"
      ],
      "metadata": {
        "id": "Dlz-2k8Yxi9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset function\n",
        "def read_dataset(inFile):\n",
        "    print(\"\\nReading:\", inFile)\n",
        "    data =  pd.read_json(inFile, lines=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "sUNo2JZIxe-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subtask = \"A\"\n",
        "\n",
        "if subtask == \"A\":\n",
        "  # data paths and config\n",
        "  inTrain = '/content/SemEval2024-Task8/SubtaskA/subtaskA_train_monolingual.jsonl'\n",
        "  inTest = '/content/SemEval2024-Task8/SubtaskA/subtaskA_dev_monolingual.jsonl'\n",
        "  inDatasetTest = '/content/mount/dataset.jsonl'\n",
        "\n",
        "  target = \"label\"\n",
        "elif subtask == \"B\":\n",
        "  # data paths and config\n",
        "  inTrain = '/content/SemEval2024-Task8/SubtaskB/subtaskB_train.jsonl'\n",
        "  inTest = '/content/SemEval2024-Task8/SubtaskB/subtaskB_dev.jsonl'\n",
        "  inDatasetTest = '/content/mount/dataset.jsonl'\n",
        "\n",
        "  target = \"model\"\n",
        "else:\n",
        "  logging.error(\"Wrong subtask: {}. It should be A or B\".format(subtask))\n",
        "  raise ValueError(\"Wrong subtask: {}. It should be A or B\".format(subtask))"
      ],
      "metadata": {
        "id": "ArWHikWxKOLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data paths and config\n",
        "\n",
        "max_instances_per_class = 2000\n",
        "max_features = 2000 # maximum number of features extracted for our instances\n",
        "random_seed = 777 # set random seed for reproducibility"
      ],
      "metadata": {
        "id": "vd8G9xDLxm3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "train_df = read_dataset(inTrain)\n",
        "test_df = read_dataset(inTest)\n",
        "dataset_df = read_dataset(inDatasetTest)"
      ],
      "metadata": {
        "id": "wRsW-zbuxotA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downsample training data to train faster\n",
        "train_df = train_df.groupby(target).sample(n=max_instances_per_class, random_state=random_seed)"
      ],
      "metadata": {
        "id": "g42gKS7nxr2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df[\"text\"]\n",
        "X_test = test_df[\"text\"]\n",
        "X_dataset = dataset_df[\"text\"]"
      ],
      "metadata": {
        "id": "hj0ba-RUxv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize labels : from text to numeric vectors\n",
        "le = LabelEncoder()\n",
        "Y_train = le.fit_transform(train_df[target])\n",
        "Y_test = le.transform(test_df[target])\n",
        "Y_dataset = le.transform(dataset_df[target])"
      ],
      "metadata": {
        "id": "v3IYVn2Ox7IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_args = {\n",
        "  'tfidf__max_features': [2000],\n",
        "  'tfidf__stop_words': [\"english\"],\n",
        "  'tfidf__ngram_range': [(1,1),(1,2)],\n",
        "  'tfidf__use_idf': [True, False],\n",
        "}\n",
        "models = {\n",
        "  'BernoulliNB': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', BernoulliNB()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__alpha': [1.0, 0.01, 10.0],\n",
        "        'clf__fit_prior': [True, False]\n",
        "      },\n",
        "  },\n",
        "  'MultinomialNB': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', MultinomialNB()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__alpha': [1.0, 0.01, 10.0],\n",
        "        'clf__fit_prior': [True, False]\n",
        "      },\n",
        "  },\n",
        "  'ExtraTreesClassifier': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', ExtraTreesClassifier()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__n_estimators': [100],\n",
        "        'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
        "      },\n",
        "  },\n",
        "  'PassiveAggressiveClassifier': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', PassiveAggressiveClassifier()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__C': [1.0, 0.5, 10.0],\n",
        "        'clf__max_iter': [1000, 500, 2000],\n",
        "      },\n",
        "  },\n",
        "  'LinearSVC': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LinearSVC()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__penalty': ['l1', 'l2'],\n",
        "        'clf__max_iter': [1000, 500, 2000],\n",
        "      },\n",
        "  },\n",
        "  'NearestCentroid': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', NearestCentroid()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__metric': ['euclidean', 'manhattan'],\n",
        "      },\n",
        "  },\n",
        "  'Perceptron': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', Perceptron()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'clf__alpha': [0.0001, 0.1, 1.0],\n",
        "        'clf__max_iter': [1000, 500, 2000],\n",
        "      },\n",
        "  },\n",
        "  'ComplementNB': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', ComplementNB()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__alpha': [1.0, 0.01, 10.0],\n",
        "        'clf__norm': [True, False],\n",
        "      },\n",
        "  },\n",
        "  'KNeighborsClassifier': {\n",
        "      'pipeline': Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', KNeighborsClassifier()),\n",
        "      ]),\n",
        "      'args': {\n",
        "        'clf__n_neighbors': [5, 10, 2],\n",
        "        'clf__weights': ['uniform', 'distance'],\n",
        "      },\n",
        "  },\n",
        "}"
      ],
      "metadata": {
        "id": "68XDsYRQzjKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(name, values):\n",
        "  args = values['args'].update(tfidf_args)\n",
        "  print(f\"Optimizando {name}\")\n",
        "  grid_search = GridSearchCV(values['pipeline'], values['args'], scoring='f1_macro', n_jobs=-1)\n",
        "  grid_search.fit(X_train, Y_train)\n",
        "  print(f'parameters: {grid_search.best_params_}')\n",
        "\n",
        "  y_pred = grid_search.predict(X_test)\n",
        "  score = f1_score(Y_test, y_pred, average=\"macro\")\n",
        "  print(f\"Macro F1: {score}\")\n",
        "  print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "EFdsbv1zUEsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, values in models.items():\n",
        "  fine_tune(name, values)"
      ],
      "metadata": {
        "id": "G_nFm3GX4WRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = models['NearestCentroid']\n",
        "fine_tune('NearestCentroid', values)"
      ],
      "metadata": {
        "id": "xvwOi1A2UTfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hxdlMXCb1sqv"
      }
    }
  ]
}