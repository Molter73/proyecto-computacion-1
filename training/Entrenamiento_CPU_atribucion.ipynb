{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMm6Y72THSiUiu/NLko9f46"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpJ73KFoxUFf"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==v4.6.3\n",
        "\n",
        "![ ! -d /content/SemEval2024-Task8 ] && gdown --folder https://drive.google.com/drive/folders/14DulzxuH5TDhXtviRVXsH5e2JTY2POLi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "id": "mH66TNaQxVPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from joblib import parallel_backend\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FormatStrFormatter"
      ],
      "metadata": {
        "id": "ey9Yt12ZyKXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset function\n",
        "def read_dataset(inFile):\n",
        "    print(\"\\nReading:\", inFile)\n",
        "    data =  pd.read_json(inFile, lines=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "qKUC1RZFxYYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(vectorizer, x_train, x_test, x_dataset):\n",
        "  return (\n",
        "      vectorizer.fit_transform(x_train),\n",
        "      vectorizer.transform(x_test),\n",
        "      vectorizer.transform(x_dataset),\n",
        "  )"
      ],
      "metadata": {
        "id": "ScUTVppp1sAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X, y, x_test, y_test, x_dataset, y_dataset, name=None):\n",
        "  if name is None:\n",
        "    name = model.__class__.__name__\n",
        "\n",
        "  model.fit(X, y)\n",
        "\n",
        "  y_pred = model.predict(x_test)\n",
        "  score = f1_score(y_test, y_pred, average=\"macro\")\n",
        "  print(f\"Macro {name} F1: {score}\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  test_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "  y_pred = model.predict(x_dataset)\n",
        "  score = f1_score(y_dataset, y_pred, average=\"macro\")\n",
        "  print(f\"Macro {name} dataset F1: {score}\")\n",
        "  print(classification_report(y_dataset, y_pred))\n",
        "\n",
        "  if hasattr(model, 'best_params_'):\n",
        "    print(f'Mejores parámetros: {model.best_params_}')\n",
        "\n",
        "  return {\n",
        "      'test': test_report,\n",
        "      'dataset': classification_report(y_dataset, y_pred, output_dict=True),\n",
        "      'best_params': model.best_params_ if hasattr(model, 'best_params_') else None,\n",
        "  }"
      ],
      "metadata": {
        "id": "u98VQGvHx1gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_stats(df, id2label):\n",
        "  #Establecemos el número de instancias presentes\n",
        "  instancias_humanas = len(df[df['label'] == 0])\n",
        "  instancias_ia =  len(df[df['label'] == 1])\n",
        "  instancias_dataset = len(df)\n",
        "\n",
        "  #Sumamos las instancias y realizamos la longitud media\n",
        "  longitudes_medias = []\n",
        "  for label in id2label:\n",
        "    acc = df[df['model'] == label]['text'].apply(len).sum()\n",
        "    length = len(df[df['model'] == label])\n",
        "    if length != 0:\n",
        "      longitudes_medias.append(acc / length)\n",
        "    else:\n",
        "      longitudes_medias.append(0)\n",
        "\n",
        "  #Imprimimos la Tabla de Estadísticas\n",
        "  print('Número de instancias en el dataset:\\t\\t\\t\\t', instancias_dataset)\n",
        "\n",
        "  for i in range(len(id2label)):\n",
        "    label = id2label[i]\n",
        "    print(f'Número de instancias {label}:\\t\\t\\t\\t\\t', len(df[df['model'] == label]))\n",
        "\n",
        "  for i in range(len(id2label)):\n",
        "    label = id2label[i]\n",
        "    print(f'Longitud media en caracteres de las instancias {label}:\\t\\t', longitudes_medias[i])"
      ],
      "metadata": {
        "id": "YOJKopP2gLmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data paths and config\n",
        "inTrain = '/content/SemEval2024-Task8/SubtaskB/subtaskB_train.jsonl'\n",
        "inTest = '/content/SemEval2024-Task8/SubtaskB/subtaskB_dev.jsonl'\n",
        "inDatasetTest = '/content/mount/dataset.jsonl'\n",
        "\n",
        "max_instances_per_class = 2000\n",
        "max_features = 20000 # maximum number of features extracted for our instances\n",
        "random_seed = 777 # set random seed for reproducibility\n",
        "\n",
        "results = {\n",
        "}"
      ],
      "metadata": {
        "id": "NQS5FBhAxbv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelos a evaluar\n",
        "models = [\n",
        "  ExtraTreesClassifier(),\n",
        "  GradientBoostingClassifier(),\n",
        "  RandomForestClassifier(),\n",
        "  AdaBoostClassifier(),\n",
        "  BernoulliNB(),\n",
        "]"
      ],
      "metadata": {
        "id": "aBSvYx76yYWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = read_dataset(inTrain)\n",
        "test_df = read_dataset(inTest)\n",
        "dataset_df = read_dataset(inDatasetTest)"
      ],
      "metadata": {
        "id": "IVGhw6qFxdcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize labels : from text to numeric vectors\n",
        "le = LabelEncoder()\n",
        "Y_train = le.fit_transform(train_df[\"model\"])"
      ],
      "metadata": {
        "id": "Ebn6dMhN0BQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_stats(train_df, le.classes_)"
      ],
      "metadata": {
        "id": "N-cZbCXPgPk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.groupby(\"model\").sample(n=max_instances_per_class, random_state=random_seed)"
      ],
      "metadata": {
        "id": "YZ5Z7dIjxiPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize labels : from text to numeric vectors\n",
        "le = LabelEncoder()\n",
        "Y_train = le.fit_transform(train_df[\"model\"])\n",
        "Y_test = le.transform(test_df[\"model\"])\n",
        "Y_dataset = le.transform(dataset_df[\"model\"])"
      ],
      "metadata": {
        "id": "bPYgA0FM1o03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_stats(train_df, le.classes_)"
      ],
      "metadata": {
        "id": "1BAo99HIgTky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_stats(test_df, le.classes_)"
      ],
      "metadata": {
        "id": "M-0Rtr3-gUGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_stats(dataset_df, le.classes_)"
      ],
      "metadata": {
        "id": "MZ1fi3sRgUfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize data: extract features from our data (from text to numeric vectors)\n",
        "vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "X_train, X_test, X_dataset = vectorize(vectorizer, train_df[\"text\"], test_df[\"text\"], dataset_df[\"text\"])"
      ],
      "metadata": {
        "id": "4kBZ6q1vxl-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with parallel_backend('threading', n_jobs=multiprocessing.cpu_count()):\n",
        "  for model in models:\n",
        "    results[model.__class__.__name__] = {}\n",
        "    results[model.__class__.__name__]['baseline'] = evaluate(model, X_train, Y_train, X_test, Y_test, X_dataset, Y_dataset)"
      ],
      "metadata": {
        "id": "jJVCgdpLyW0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize data: extract features from our data (from text to numeric vectors)\n",
        "vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
        "X_train_stopwords, X_test_stopwords, X_dataset_stopwords = vectorize(vectorizer, train_df[\"text\"], test_df[\"text\"], dataset_df[\"text\"])"
      ],
      "metadata": {
        "id": "URxAC75Z0j21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with parallel_backend('threading', n_jobs=multiprocessing.cpu_count()):\n",
        "  for model in models:\n",
        "    results[model.__class__.__name__]['stopwords'] = evaluate(model, X_train_stopwords, Y_train, X_test_stopwords, Y_test, X_dataset_stopwords, Y_dataset)"
      ],
      "metadata": {
        "id": "PaLRMEV12oDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize data: extract features from our data (from text to numeric vectors)\n",
        "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1,3))\n",
        "X_train_ngrams, X_test_ngrams, X_dataset_ngrams = vectorize(vectorizer, train_df[\"text\"], test_df[\"text\"], dataset_df[\"text\"])"
      ],
      "metadata": {
        "id": "JpFzYgZ_4D0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with parallel_backend('threading', n_jobs=multiprocessing.cpu_count()):\n",
        "  for model in models:\n",
        "    results[model.__class__.__name__]['ngrams'] = evaluate(model, X_train_ngrams, Y_train, X_test_ngrams, Y_test, X_dataset_ngrams, Y_dataset)"
      ],
      "metadata": {
        "id": "Cw6DfyOp4atz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  'ExtraTreesClassifier': {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "  },\n",
        "  'GradientBoostingClassifier': {\n",
        "      'loss': ['log_loss', 'exponential'],\n",
        "      'criterion': ['friedman_mse', 'squared_error'],\n",
        "  },\n",
        "  'RandomForestClassifier': {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "  },\n",
        "  'AdaBoostClassifier': {\n",
        "      'estimator': [None, ExtraTreesClassifier(), ExtraTreesClassifier(criterion='log_loss', n_estimators=150)],\n",
        "  },\n",
        "  'BernoulliNB': {\n",
        "      'alpha': [1.0, 0.01, 10.0],\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "nSn5LWzTJair"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize data: extract features from our data (from text to numeric vectors)\n",
        "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1,3))\n",
        "X_train_cv, X_test_cv, X_dataset_cv = vectorize(vectorizer, train_df[\"text\"], test_df[\"text\"], dataset_df[\"text\"])"
      ],
      "metadata": {
        "id": "yV50N94xT4h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with parallel_backend('threading', n_jobs=multiprocessing.cpu_count()):\n",
        "  for model in models:\n",
        "    name = model.__class__.__name__\n",
        "    results[name]['cv'] = evaluate(GridSearchCV(model, params[name], scoring='f1_macro'), X_train_cv, Y_train, X_test_cv, Y_test, X_dataset_cv, Y_dataset, name=name)"
      ],
      "metadata": {
        "id": "R0L1YyloT8Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreamos modelos con parámetros optimizados para el entrenamiento final\n",
        "models = [\n",
        "  ExtraTreesClassifier(**results['ExtraTreesClassifier']['cv']['best_params']),\n",
        "  GradientBoostingClassifier(**results['GradientBoostingClassifier']['cv']['best_params']),\n",
        "  RandomForestClassifier(**results['RandomForestClassifier']['cv']['best_params']),\n",
        "  AdaBoostClassifier(**results['AdaBoostClassifier']['cv']['best_params']),\n",
        "  BernoulliNB(**results['BernoulliNB']['cv']['best_params']),\n",
        "]"
      ],
      "metadata": {
        "id": "YCLsffmvXCF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = read_dataset(inTrain)\n",
        "test_df = read_dataset(inTest)\n",
        "dataset_df = read_dataset(inDatasetTest)"
      ],
      "metadata": {
        "id": "vpIz7L30P9GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subimos a 9k instancias\n",
        "train_df = train_df.groupby(\"model\").sample(n=5000, random_state=random_seed)\n",
        "\n",
        "# vectorize labels : from text to numeric vectors\n",
        "le = LabelEncoder()\n",
        "Y_train = le.fit_transform(train_df[\"model\"])\n",
        "Y_test = le.transform(test_df[\"model\"])\n",
        "Y_dataset = le.transform(dataset_df[\"model\"])\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1,3))\n",
        "X_train_full, X_test_full, X_dataset_full = vectorize(vectorizer, train_df[\"text\"], test_df[\"text\"], dataset_df[\"text\"])"
      ],
      "metadata": {
        "id": "n0ugVaMFOyEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_stats(train_df, le.classes_)"
      ],
      "metadata": {
        "id": "4EvHoPyugo-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with parallel_backend('threading', n_jobs=multiprocessing.cpu_count()):\n",
        "  for model in models:\n",
        "    results[model.__class__.__name__]['full-datasets'] = evaluate(model, X_train_full, Y_train, X_test_full, Y_test, X_dataset_full, Y_dataset)"
      ],
      "metadata": {
        "id": "XN2CQkvUPY5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(model_names, scores, title):\n",
        "  x = np.arange(len(model_names)) # the label locations\n",
        "  width = 0.15  # the width of the bars\n",
        "  multiplier = 0\n",
        "\n",
        "  fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "  for test, results in scores.items():\n",
        "    offset = width * multiplier\n",
        "    rects = ax.bar(x + offset, results, width, label=test)\n",
        "    ax.bar_label(rects, padding=3, labels=[f'{r:.2f}' for r in results], rotation='vertical')\n",
        "    multiplier += 1\n",
        "\n",
        "  ax.set_title(title)\n",
        "  ax.set_xticks(x + width, model_names, rotation=30)\n",
        "  ax.legend(loc='upper left', ncols=3)\n",
        "  ax.set_ylim(0, 1.2)\n",
        "\n",
        "  plt.savefig(os.path.join('/content/mount', 'subtaskB', f'{title}.png'))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "HsC8uZMx4tAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = {\n",
        "  'f1': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "  'accuracy': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "  'precision': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "  'recall': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "}\n",
        "model_names = []\n",
        "\n",
        "for model, tests in results.items():\n",
        "  model_names.append(model)\n",
        "  for test, result in tests.items():\n",
        "    scores['f1'][test].append(result['test']['macro avg']['f1-score'])\n",
        "    scores['accuracy'][test].append(result['test']['accuracy'])\n",
        "    scores['precision'][test].append(result['test']['macro avg']['precision'])\n",
        "    scores['recall'][test].append(result['test']['macro avg']['recall'])"
      ],
      "metadata": {
        "id": "6M2C3okv9Emh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['f1'], 'F1 macro scores')"
      ],
      "metadata": {
        "id": "W_xZ2fQT7pkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['accuracy'], 'Accuracy scores')"
      ],
      "metadata": {
        "id": "FR3c313KbO99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['precision'], 'Precision scores')"
      ],
      "metadata": {
        "id": "FjLkSRr9bcks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['recall'], 'Recall scores')"
      ],
      "metadata": {
        "id": "q7egGaIzbmQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_export = [\n",
        "  {\n",
        "      'Modelo': model_names[i],\n",
        "      'Accuracy': scores['accuracy']['full-datasets'][i],\n",
        "      'Precision': scores['precision']['full-datasets'][i],\n",
        "      'Recall': scores['recall']['full-datasets'][i],\n",
        "      'F1-score': scores['f1']['full-datasets'][i],\n",
        "  } for i in range(len(model_names))\n",
        "]\n",
        "pd.DataFrame(text_export).to_csv('/content/mount/TaskB_SemEval.csv', index=False)"
      ],
      "metadata": {
        "id": "17ZkqyO4xNOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = {\n",
        "  'f1': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "  'accuracy': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "  'precision': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "  'recall': {\n",
        "    'baseline': [],\n",
        "    'stopwords': [],\n",
        "    'ngrams': [],\n",
        "    'cv': [],\n",
        "    'full-datasets': [],\n",
        "  },\n",
        "}\n",
        "model_names = []\n",
        "\n",
        "for model, tests in results.items():\n",
        "  model_names.append(model)\n",
        "  for test, result in tests.items():\n",
        "    scores['f1'][test].append(result['dataset']['macro avg']['f1-score'])\n",
        "    scores['accuracy'][test].append(result['dataset']['accuracy'])\n",
        "    scores['precision'][test].append(result['dataset']['macro avg']['precision'])\n",
        "    scores['recall'][test].append(result['dataset']['macro avg']['recall'])"
      ],
      "metadata": {
        "id": "Y364GqtsLJzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['f1'], 'F1 macro scores')"
      ],
      "metadata": {
        "id": "dud09BtKX06f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['accuracy'], 'Accuracy scores')"
      ],
      "metadata": {
        "id": "CgpheMT4b4z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['precision'], 'Precision scores')"
      ],
      "metadata": {
        "id": "TAESCYS2b7FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(model_names, scores['recall'], 'Recall scores')"
      ],
      "metadata": {
        "id": "Jrehfnqkb88y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_export = [\n",
        "  {\n",
        "      'Modelo': model_names[i],\n",
        "      'Accuracy': scores['accuracy']['full-datasets'][i],\n",
        "      'Precision': scores['precision']['full-datasets'][i],\n",
        "      'Recall': scores['recall']['full-datasets'][i],\n",
        "      'F1-score': scores['f1']['full-datasets'][i],\n",
        "  } for i in range(len(model_names))\n",
        "]\n",
        "pd.DataFrame(text_export).to_csv('/content/mount/TaskB_Dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "mAzjCwmLb_gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "subtask=\"B\"\n",
        "\n",
        "os.makedirs(f'/content/mount/models{subtask}', exist_ok=True)\n",
        "\n",
        "for model in models:\n",
        "  name = model.__class__.__name__\n",
        "  with open(f'/content/mount/models{subtask}/{name}.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "with open(f'/content/mount/vectorizer{subtask}.pkl', 'wb') as f:\n",
        "  pickle.dump(vectorizer, f)\n",
        "\n",
        "with open(f'/content/mount/labels{subtask}.pkl', 'wb') as f:\n",
        "  pickle.dump(le.classes_, f)"
      ],
      "metadata": {
        "id": "hva2aw04gBbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgMQEI4mtM1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}